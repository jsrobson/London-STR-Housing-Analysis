{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6ybEsGDrQLXUXxUM/uYuY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsrobson/2023-cis5450-termproject/blob/main/CIS5450_TermProject_01_acquire_data_COMPLETE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook: Data Acquisition\n",
        "This notebook captures the various efforts made by project team members with to acquire and parse data during the earliest phase of the project in late October and early November 2023 as the project proposal was being developed. These efforts were deprecated as new and better data sources were found. Nevertheless, these efforts reflect a foundational part of building the team's understanding of how to approach the work and merit **archival** retention.\n",
        "\n",
        "**Note:** Each cell is annotated with a brief description of the code, its intended purpose, and its output. As these code pieces were intended to collect data from external sources or clean collected data for analysis and were part of larger codebases, they are not intended to be run within this notebook and are included for posterity only."
      ],
      "metadata": {
        "id": "jIY3gUTsAY6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Description"
      ],
      "metadata": {
        "id": "linOUULhS89e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Primary Data Parsing:** Prior to the discovery of an attribute-linked property dataset, several investigations were performed on [Price Paid data](https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads) maintained by the United Kingdom (UK) government to determine its suitability with respect to the project requirements. Two different approaches to data wrangling were trialled using Dask to read a large file (.csv format) of property data for England and Wales for the initial 2019 study period:\n",
        "* selecting pre-filtered data for the initial 2019 study period and parsing further using hardcoded columnar names, and\n",
        "* using XPath on the Price Paid data website to automatically read file headers into the file reading process.\n",
        "\n",
        "These approaches were each trialled to determine their impact on the resulting dataframe. All were deprecated with the introduction of EPDC data acquisition and wrangling challenges, discussed further in **EPDC Data Acquisition**."
      ],
      "metadata": {
        "id": "lZBFAaGBTdR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# function reads property file to dask dataframe\n",
        "def read_property_to_df(filepath, boroughs):\n",
        "    # generate and sort list of filenames\n",
        "    filenames = glob.glob(filepath + \"/*.csv\")\n",
        "    filenames.sort()\n",
        "    # initialize cols for dataframe\n",
        "    cols = ['unique_id', 'price_paid', 'purchase_date', 'postcode',\n",
        "            'property_type', 'new_build', 'estate_type', 'saon', 'paon',\n",
        "            'street', 'locality', 'town', 'district', 'county',\n",
        "            'transaction_category', 'ext']\n",
        "    # initialize empty list of dataframes for consolidation\n",
        "    dataframes = []\n",
        "    # for each filename in list of filenames\n",
        "    for filename in filenames:\n",
        "        print(\"Parsing file \", filename)\n",
        "        # get dask dataframe associated with filename\n",
        "        file_in = dd.read_csv(filename, dtype='string', keep_default_na=False)\n",
        "        # append dask dataframe to list\n",
        "        dataframes.append(parse_property_segment(file_in, cols))\n",
        "    # concatenate dataframes in list and reset index\n",
        "    concat_df = dd.concat(dataframes).reset_index(drop=True)\n",
        "    # return concatenated dataframe with index reset\n",
        "    return concat_df.loc[concat_df['district'].str.lower().isin([\n",
        "        borough.lower() for borough in boroughs])]\n",
        "\n",
        "# function parses a given property dataframe with pre-set column names\n",
        "def parse_property_segment(dataframe, col_names):\n",
        "    # set dataframe column headers to col_names\n",
        "    dataframe.columns = col_names\n",
        "    # drop unneeded columns\n",
        "    to_drop = ['transaction_category', 'ext']\n",
        "    file_in = dataframe.drop(columns=to_drop)\n",
        "    # cast price_paid as float\n",
        "    file_in['price_paid'] = file_in['price_paid'].astype(float)\n",
        "    # convert purchase_date as date/time\n",
        "    file_in['purchase_date'] = dd.to_datetime(file_in['purchase_date'],\n",
        "                                              format='%Y-%m-%d %H:%M')\n",
        "    # get year, month, day as individual cols\n",
        "    file_in['year'] = file_in['purchase_date'].dt.year.astype('int32')\n",
        "    file_in['month'] = file_in['purchase_date'].dt.month.astype('int32')\n",
        "    file_in['day'] = file_in['purchase_date'].dt.day.astype('int32')\n",
        "    # return dataframe filtered by county being Greater London\n",
        "    return file_in[file_in['county'] == 'GREATER LONDON']"
      ],
      "metadata": {
        "id": "1JSKX1kMVWyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download file tempo\n",
        "!wget -nc http://prod2.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2019-part1.csv\n",
        "#Webpage with info of table\n",
        "# make request to target webpage\n",
        "w = requests.get(\"https://www.gov.uk/guidance/about-the-price-paid-data#explanations-of-column-headers-in-the-ppd\")\n",
        "# initialize dom tree object\n",
        "dom_tree = html.fromstring(w.content)\n",
        "# instantiate identified xpath string\n",
        "table_body_xpath  = \"/html/body/div[3]/main/div[4]/div[1]/div/div[3]/div/table/tbody\"\n",
        "\n",
        "# Get headers for csv using xpath\n",
        "headers = dom_tree.xpath(table_body_xpath + \"/tr/th/text()\")\n",
        "# place headers in list\n",
        "cleaned_headers = [item.strip() for item in headers if item.strip()]\n",
        "\n",
        "# Get description of header\n",
        "description_header = dom_tree.xpath(table_body_xpath + \"/tr\")\n",
        "\n",
        "# build headers list\n",
        "description_header_list = []\n",
        "for row in description_header:\n",
        "    row_data = row.xpath(\"td//text()\")\n",
        "    description_header_list.append(row_data)\n",
        "\n",
        "# Create a DataFrame of Header and description\n",
        "header_df = pd.DataFrame({'Header': cleaned_headers, 'Description': description_header_list})\n",
        "\n",
        "# Read the CSV file with the header\n",
        "price_paid_df = pd.read_csv(\"pp-2019-part1.csv\", header=None, names=cleaned_headers)"
      ],
      "metadata": {
        "id": "ki14597LV6YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Secondary Data Parsing**: As part of early data investigations, public and open source data for [private](https://data.london.gov.uk/dataset/local-authority-average-rents) and [social](https://data.london.gov.uk/dataset/local-authority-average-rents) housing rents were sourced as .csv from the London Datastore (Mayor of London / London Assembly), a central repository for urban data pertaining for Greater London.\n",
        "\n",
        "It was collectively decided that purchase price exhibited a more direct relationship to the project problem statement. At time of decision, the team had written scratch methods to read and parse the specified files.\n",
        "\n"
      ],
      "metadata": {
        "id": "bSHFprbCEy7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kuNf3QD_1II"
      },
      "outputs": [],
      "source": [
        "# helper method reads and parses rent data for private housing stock, by borough\n",
        "# returns pandas dataframe\n",
        "def read_rents_private_df():\n",
        "    # assign dtypes to each column\n",
        "    d_rents = {\n",
        "        \"borough\": \"str\",\n",
        "        \"lower_quartile\": \"int32\",\n",
        "        \"median\": \"int32\",\n",
        "        \"upper_quartile\": \"int32\"\n",
        "    }\n",
        "    # read file\n",
        "    file_in = pd.read_csv(\"resources/ldn_rent2023.csv\",\n",
        "                          dtype=d_rents)\n",
        "    # standardize borough names by swapping ampersand with and\n",
        "    file_in['borough'] = file_in['borough'].str.replace(\"&\", \"and\")\n",
        "    return file_in\n",
        "\n",
        "\n",
        "# helper method reads and parses rent data for social housing stock, by borough\n",
        "# returns pandas dataframe\n",
        "def read_rents_social():\n",
        "    # read file\n",
        "    file_in = pd.read_csv(\"resources/ldn_rentsoc.csv\")\n",
        "    # replace any incomplete element with none\n",
        "    file_in = file_in.replace({'LSVT': None, '..': None, '.': None})\n",
        "    # drop nulls and unnecessary columns\n",
        "    file_in.dropna(inplace=True)\n",
        "    file_in.drop(axis=1, columns=['Code', 'New Code'], inplace=True)\n",
        "    # get column names not related to area\n",
        "    columns = [i for i in file_in.columns if i not in ['Area']]\n",
        "    # convert non-string columns to float\n",
        "    for col in columns:\n",
        "        file_in[col] = pd.to_numeric(file_in[col], errors='ignore')\n",
        "    # rename column for common alignment\n",
        "    file_in.rename(inplace=True, columns={'Area': 'borough'})\n",
        "    return file_in"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EPDC Data Acquisition**: While the Price Paid dataset is an important and rich dataset, it lacks a crucial means of standardization: a corresponding floor area for each listed property to derive a relative cost-per-square-metre calculation.\n",
        "\n",
        "The UK Government also maintains stores of [environmental performance (EPDC) data](https://www.gov.uk/government/statistical-data-sets/live-tables-on-energy-performance-of-buildings-certificates) for buildings that are updated whenever a building in the UK is leased or sold. Crucially, the data includes a square metre feature `tfarea`.\n",
        "\n",
        "Though the data is freely available, it is limited to 5000 records per query when using the web-based querying tool. This motivated a need to pursue alternative code-based approaches to data acquisition using the Python [requests](https://requests.readthedocs.io/en/latest/) library. The team wrote methods to access EPDC data batched by quarter and year for each borough in Greater London. Preliminary work was performed on the collected data to join with housing purchase records wherever possible.\n",
        "\n",
        "The discovery of an [attribute-linked dataset](https://reshare.ukdataservice.ac.uk/854942/) prepared by University College London (UCL) researchers that joined these two disparate datasets limited the utility of this work. As our joined dataset exhibited drastically fewer rows after the merge of purchase price and environmental performance data, it was decided to use the more stable UCL dataset release.\n"
      ],
      "metadata": {
        "id": "mIPxdiMXGySg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import statements\n",
        "import requests\n",
        "from requests.auth import HTTPBasicAuth\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# code gets UK domestic environmental performance data for housing\n",
        "# from web server using Python requests library\n",
        "# (https://requests.readthedocs.io/en/latest/)\n",
        "\n",
        "# helper method uses existing data source to build\n",
        "# dict of k: borough / v: code (i.e. numeric code corresponding to borough)\n",
        "def borough_dict():\n",
        "    # create dataframe from social rents\n",
        "    df = pd.read_csv('resources/ldn_rentsoc.csv')\n",
        "    # get borough names and codes as lists\n",
        "    boroughs = df['Area'].tolist()\n",
        "    codes = df['New Code'].tolist()\n",
        "    # create dictionary from borough name (k) - code (v) pair\n",
        "    return {boroughs[i]: codes[i] for i in range(len(boroughs))}\n",
        "\n",
        "\n",
        "# helper method initializes time segment data structures to\n",
        "# organize data gained from web\n",
        "def time_ds():\n",
        "    # initialize year and month ranges\n",
        "    years = ['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018',\n",
        "             '2019', '2020', '2021', '2022']\n",
        "    batch = [('1', '3'), ('4', '6'), ('7', '9'), ('10', '12')]\n",
        "    # return as tuple\n",
        "    return years, batch\n",
        "\n",
        "\n",
        "# main method, get epdc data using requests library\n",
        "def main():\n",
        "    # get borough codes, years, and month ranges\n",
        "    b_codes = borough_dict()\n",
        "    years, batch = time_ds()\n",
        "    # initialize api key (scrubbed for public view)\n",
        "    key = '<REMOVED>'\n",
        "    # use requests basic authentication\n",
        "    basic = HTTPBasicAuth('jsrobson@seas.upenn.edu', key)\n",
        "    # set headers\n",
        "    headers = {'Accept': 'text/csv'}\n",
        "    # initialize url stub and related text strings\n",
        "    url_stub = 'https://epc.opendatacommunities.org/api/v1/domestic/search'\n",
        "    l_auth = 'local-authority='\n",
        "    year_f = ('from-year=', 'from-month=')\n",
        "    year_t = ('to-year=', 'to-month=')\n",
        "    # for each borough in dict, corresponding code\n",
        "    for borough, code in b_codes.items():\n",
        "        # for each year in 2011 - 2022\n",
        "        for year in years:\n",
        "            # print message for reference\n",
        "            print(\"Getting \" + year + \" for \" + borough)\n",
        "            # for each bi-annual batch\n",
        "            for tup in batch:\n",
        "                # generate url stub\n",
        "                url = (url_stub + '?' + l_auth + code + '&' + year_f[1] +\n",
        "                       tup[0] + '&' + year_f[0] + year + '&' + year_t[1] +\n",
        "                       tup[1] + '&' + year_t[0] + year + '&size=10000')\n",
        "                # get url using requests\n",
        "                r = requests.get(url, auth=basic, headers=headers)\n",
        "                # generate filename title\n",
        "                title = (borough + '_' + year + '_' + tup[0] + '_' + tup[1] +\n",
        "                         '.csv')\n",
        "                # print url for reference\n",
        "                print(url)\n",
        "                # # save data as csv at title\n",
        "                open('output/' + title, 'wb').write(r.content)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "SroMNApOGxdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EPDC Data Cleaning:** Once the EPDC data was collected using the above requests-driven process, two functions were written to read the data from .csv format into a Pandas dataframe and parse it according to best practices.\n",
        "\n",
        "This code was used to test initial viability of including EPDC data in the overall project dataset and, as above, was deprecated when the UCL attribute-linked dataset was sourced."
      ],
      "metadata": {
        "id": "chrei8MzNQoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# function reads EPDC data at filepath into pandas dataframe,\n",
        "# returns cumulative EPDC dataframe using helper function\n",
        "def read_epdc_to_df(filepath):\n",
        "    # generate and sort list of filenames\n",
        "    filenames = glob.glob(filepath + \"/*.csv\")\n",
        "    filenames.sort()\n",
        "    # initialize empty list of dataframes for consolidation\n",
        "    dataframes = []\n",
        "    # for each available file\n",
        "    for filename in filenames:\n",
        "        # append the parsed dataframe chunk into array of df\n",
        "        dataframes.append(parse_epdc_segment(filename))\n",
        "    # return concatenated cumulative df from chunked dfs\n",
        "    return dd.concat(dataframes).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# function reads EPDC data segment at filepath into pandas dataframe\n",
        "# and parses according to anticipated uses for data and best practices\n",
        "def parse_epdc_segment(filepath):\n",
        "    # generate dataframe from csv at path\n",
        "    file_in = dd.read_csv(filepath, dtype='string', keep_default_na=False)\n",
        "    # identify columns to keep\n",
        "    to_keep = ['lmk-key', 'address1', 'address2', 'address3', 'postcode',\n",
        "               'property-type', 'built-form', 'local-authority', 'tenure',\n",
        "               'total-floor-area', 'uprn']\n",
        "    # drop unneeded columns\n",
        "    file_in = file_in[to_keep]\n",
        "    # drop any column where uprn is empty\n",
        "    file_in = file_in[file_in['uprn'] != '']\n",
        "    # cast total-floor-area as float and uprn as int\n",
        "    file_in['total-floor-area'] = file_in['total-floor-area'].astype(float)\n",
        "    file_in['uprn'] = file_in['uprn'].astype(int)\n",
        "    # return dataframe\n",
        "    return file_in.sort_values(by='uprn', ascending=True)"
      ],
      "metadata": {
        "id": "05lye-86Occt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Developing an appropriate join key:** With both price paid and EPDC data collected, it became a matter of determining how to join the data such that a property with price paid data and floor area data could be united in one location. Initially, the address of the property was considered as an obvious solution. However, an address across the two datasets was not consistent. For example, EPDC data for the London borough of Camden in 2019 contains an address in the format:\n",
        "\n",
        "\n",
        "```\n",
        "address1: Flat 86\n",
        "address2: 19-23 Fitzroy Street\n",
        "address3: ''\n",
        "```\n",
        "while the same address in the purchase paid data is arrayed as:\n",
        "\n",
        "```\n",
        "address1: 19-23\n",
        "address2: FLAT 86\n",
        "address3: FITZROY STREET\n",
        "```\n",
        "There were also inconsistencies among address series within each dataset such that resolving the different address representations across datasets was not guaranteed to yield the correct result.\n",
        "\n",
        "Mercifully, supplemental research on this issue yielded a possible solution: a [mapping](https://ubdc.ac.uk/data-services/data-catalogue/housing-data/price-paid-data-to-uprn-lookup/) of unique purchase identifier `transactionid` within purchase price data to unique property reference number `uprn` within EPDC data. This mapping was created by the [Urban Big Data Centre](https://www.ubdc.ac.uk/) (UK) using price paid and EPDC datasets for a time period of October 2008 to 2021.\n",
        "\n",
        "Code was written to read the mapping, presented in .csv format, into a dataframe for use in a dataframe merge process. The overhead to process this data motivated the search for a different approach. As above, this approach was deprecated when the UCL attribute-linked dataset was sourced.\n",
        "\n"
      ],
      "metadata": {
        "id": "j5z3eXh5Ok5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dask import dataframe as dd\n",
        "\n",
        "# helper function read the property mapping (uprn, transaction_id)\n",
        "# into a pandas dataframe\n",
        "def read_pkey_to_df(filename):\n",
        "    # allocate appropriate datatypes in dict\n",
        "    types = {\n",
        "        'uprn': 'int32',\n",
        "        'transactionid': 'string'\n",
        "    }\n",
        "    # read csv document filename into dask dataframe\n",
        "    p_key = dd.read_csv(filename, dtype=types)\n",
        "    # return only the key mapping between uprn (EPDC) and transactionid (Ppaid)\n",
        "    return p_key[['uprn', 'transactionid']]\n",
        "\n",
        "\n",
        "# given df of properties and of keys, conduct merge to integrate uprn value\n",
        "# rerturns merged pandas df\n",
        "def merge_properties_keys_df(props, keys):\n",
        "    # merge properties df and keys df with matching transactionid\n",
        "    props_m = props.merge(keys, left_on='unique_id',\n",
        "                          right_on='transactionid', how='left')\n",
        "    # drop transactionid as redundant\n",
        "    props_m = props_m.drop(columns=['transactionid'])\n",
        "    # drop any extant nulls\n",
        "    props_m = props_m.dropna()\n",
        "    # cast uprn value as integer type and return\n",
        "    props_m['uprn'] = props_m['uprn'].astype(int)\n",
        "    return props_m"
      ],
      "metadata": {
        "id": "5MQo2xvpRkxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Large file handling:** Once the UCL attribute-linked dataset was discovered, a new challenge was presented. The dataset contained our desired price paid and environmental performance data but accounted for a larger time period and geographic scale than strictly required for the problem definition, and so with it, a file size exceeding 6.0 GB.\n",
        "\n",
        "The team developed code to read the given dataset as a Dask dataframe, conduct preliminary filtering and parsing activities to control for geography and time, and save the resulting output in .csv format delimited by the updated study period of 2017-2019. This ensured that we were not working with a larger dataset than was ultimately necessary. Dask was used to quickly read the given .csv file using parallelism; the dataframe was converted to Pandas once major parallel operations were performed.\n",
        "\n",
        "This code is included as it supported creation of a new, smaller, and controlled dataset and is not persistently re-used."
      ],
      "metadata": {
        "id": "m_dsrG3VWeYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import statements\n",
        "from lh_data import read_boroughs_to_list\n",
        "from dask import dataframe as dd\n",
        "\n",
        "\n",
        "# main method\n",
        "def main():\n",
        "    # call helper method to read large file and output csv\n",
        "    read_large_file()\n",
        "\n",
        "\n",
        "# method reads large data file (> 6.0GB) and filters for\n",
        "# project parameters to reduce size.\n",
        "def read_large_file():\n",
        "    # get boroughs list\n",
        "    boroughs = read_boroughs_to_list('resources/ldn_borough.csv')\n",
        "    # convert boroughs to lowercase\n",
        "    boroughs = [borough.lower() for borough in boroughs]\n",
        "    # set dtypes to silence warning\n",
        "    dtype = {\n",
        "        'CO2_EMISS_CURR_PER_FLOOR_AREA': 'float64',\n",
        "        'LOW_ENERGY_LIGHTING': 'float64',\n",
        "        'MAIN_HEATING_CONTROLS': 'string',\n",
        "        'MULTI_GLAZE_PROPORTION': 'float64',\n",
        "        'price': 'float64',\n",
        "        'UPRN': 'float64'\n",
        "    }\n",
        "    # read target large file csv as dask dataframe\n",
        "    file = dd.read_csv(\"resources/tranall2011_19.csv\", dtype=dtype,\n",
        "                       assume_missing=True)\n",
        "    # get pandas dataframe using parsing helper method\n",
        "    df = parse_large_file(file)\n",
        "    # filter by inner london areas defined by boroughs ds\n",
        "    df = df.loc[df['district'].str.lower().isin(boroughs)]\n",
        "    # filter by year range 2017 - 2019\n",
        "    df = df[(df['year'] == 2017) | (df['year'] == 2018) | (df['year'] == 2019)]\n",
        "    # export dataframe to csv\n",
        "    df.to_csv('out.csv')\n",
        "\n",
        "\n",
        "# given a dask df derived from large csv file, helper method\n",
        "# parses and filters as needed and returns converted pandas df.\n",
        "def parse_large_file(df):\n",
        "    # convert data of transfer to_datetime file\n",
        "    df['dateoftransfer'] = dd.to_datetime(df['dateoftransfer'],\n",
        "                                          format='%Y-%m-%d')\n",
        "    # get year and convert to integer\n",
        "    df['year'] = df['dateoftransfer'].dt.year.astype('int32')\n",
        "    # build conditional statement where county contains London\n",
        "    cond_place = df['county'].str.contains('LONDON', case=False)\n",
        "    # get filtered dataframe given condition\n",
        "    f_df = df[cond_place]\n",
        "    # return dask dataframe as pandas df\n",
        "    return f_df.compute()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "QS3t26clYA-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SQL database development:** As a corollary to large file handling issues, the team weighted the merits and utility of using a structured query language (SQL) and relational database approach to data storage and management. As we continued to work with the data and introduce Dask for parallelism, it became clear that an SQL-driven approach would not ultimately be necessary and so the decision was made to maintain a Pandas environment.\n",
        "\n",
        "The below code represents preliminary SQL database development and the later creation of a function as a means to encapsulate the same effort."
      ],
      "metadata": {
        "id": "xySPxMXPzG9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uba's SQL work\n",
        "!pip install db-sqlite3\n",
        "\n",
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# Connect to SQLite database\n",
        "# to do: update path\n",
        "conn = sqlite3.connect('/content/gdrive/My Drive/temp.db')\n",
        "\n",
        "# Save the DataFrame to a table in the SQLite database\n",
        "price_paid_df.head(20).to_sql('tmep_tab', conn, index=False, if_exists='replace')\n",
        "\n",
        "# sql query to test\n",
        "query = \"SELECT Price FROM tmep_tab\"\n",
        "\n",
        "result_df = pd.read_sql_query(query, conn)\n",
        "\n",
        "# Close connection\n",
        "conn.close()\n",
        "result_df"
      ],
      "metadata": {
        "id": "j90ILAI5z-yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# condensing Uba's work into function that can be called to return df from sql db\n",
        "def convert_df_to_sql_db(db_filepath, dataframe, query):\n",
        "    # create a connection to database\n",
        "    connect = sqlite3.connect(db_filepath)\n",
        "    # convert passed df to sql db\n",
        "    dataframe.to_sql('temp_table', connect, index=False, if_exists='replace')\n",
        "    # pass query and store result as dataframe\n",
        "    result_df = pd.read_sql_query(query, connect)\n",
        "    # close database connection and return result_df\n",
        "    connect.close()\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "pISOiKwO0GN9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}